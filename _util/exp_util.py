import logging
from functools import partial
from os import path
from typing import Union, Iterable

import utilx.argex as argex
import utilx.msgex as msgex
import utilx.pathex as paex
from utilx.general import setattr_if_none_or_empty__
from utilx.ioex import SimpleFileCache, Cache


def _arg_backward_compatibility_fix(args):
    name_map = (
        ('similarity', 'similarity_layer'),
        ('pool', 'pool_layer'),
        ('batch_size', 'batchsize'),
        ('test_batch_size', 'test_batchsize'),
        ('initialization', 'model_init'),
        ('no_cache', 'no_file_cache'),
        ('no_cache_instances', 'no_mem_cache'),
        ('cache_shuffle', 'shuffle_cache'),
        ('cache_compressed', 'compress_cache'),
        ('eval_when_training', 'less_eval_while_training')
    )


def get_workspace_path(data_type: str,
                       data_set_name: str,
                       data_split_name: str,
                       target_file_or_dir: Union[str, None] = None,
                       workspace_root_path: str = '.',
                       subspace_name: Union[str, None] = None,
                       abs_path=True):
    """
    Defines a 6-level workspace structure. From top-down, they are
    1) workspace root -> 2) sub workspace (optional) -> 3) data type -> 4) data set -> 5) data split -> 6) file or dir (optional).

    The **workspace root** is the root dir of the entire workspace, whose path is specified by `workspace_root_path`;
    The **sub workspace** is an optional subdir immediately under the workspace root, whose name is specified by `sub_workspace_name`;
        a typical use of sub workspaces is to serve as the workspaces for the same experiment over different versions of data;
    The **data type** is a subdir immediately under a workspace (the workspace root or a sub workspace), separating different types of experiment data;
        we recommend six types 'meta_data', 'source_data', 'data_sets', 'results', 'analysis', 'logs';
        'meta_data': data shared by all experiments;
        'source_data': the original data from which the data sets can be built
        'data_sets': the data after pre-processing, the data which the experiment code can directly apply on;
        'results': the experiment results;
        'analysis': any analysis results, e.g. analysis for the source data, or analysis for the results, or plot, etc;
        'logs': experiment code logs generated by the logging modules.
    The **data set** is a subdir immediately under the 'data-type' level dir, which saves data of the particular type related to a data set.
    The **data split** is a subdir immediately under a 'data-set' level dir, which saves a split of the data set;
        typical data splits are train/test/validation splits;
        for data with timestamps, the data splits can also be months, so that we train a model with previous months and test on the next few months.
    Finally, the optional **target file or directory**, the name of the actually file or directory to look for; if this is not specified, then we look for the path to the data-split level directory.

    :param data_type: the data-type level directory name; e.g. 'meta_data', 'source_data', 'data_sets', 'results', 'analysis', 'logs', etc.
    :param data_set_name: the data set (directory) name.
    :param data_split_name: the data split (directory) name; e.g. 'train', 'test', 'val', etc.
    :param target_file_or_dir: the name of the target file or directory under the data split directory; if looking for the path the the data-split level directory, then leave this parameter as `None`.
    :param workspace_root_path: the path to the workspace dir.
    :param subspace_name: the name for the sub-workspace; if a sub-workspace is not needed, leave this parameter as `None`.
    :param abs_path: `True` if an absolute path should always be returned; otherwise `False`.
    :return: the path to the target file or directory.
    """

    msgex.ensure_arg_not_none(arg_val=workspace_root_path, arg_name='workspace_root', extra_msg='workspace root is not provided')
    msgex.ensure_arg_not_none_or_empty(arg_val=data_type, arg_name='data_type', extra_msg='data type is not specified')
    path_str = path.join(*(x for x in (workspace_root_path, subspace_name, data_type, data_set_name, data_split_name, target_file_or_dir) if x))
    return path.abspath(path_str) if abs_path else path_str


def get_input_path(args, data_type: str, target: Union[str, Iterable] = '', multipath=False, multi_path_delimiter=paex.DEFAULT_MULTI_PATH_DELIMITER, unpack_for_single_result=True, return_first_only=False, **kwargs):
    """
    Gets path(s) from certain types of data in the workspace.
    This method assumes the workspace is organized with four levels: data_type/data_set/data_split/target.
    :param args:
    :param data_type:
    :param target:
    :param multipath: `True` if this method returns a single string as a 'multi-path'.
    :param multi_path_delimiter:
    :param unpack_for_single_result:
    :param return_first_only:
    :param kwargs:
    :return:
    """
    args = argex.update_args(args, **kwargs)
    if multipath:
        return path.join(args.workspace_root, multi_path_delimiter.join(argex.apply_arg_combo(get_workspace_path,
                                                                                              data_type=data_type,
                                                                                              data_set_name=args.data_set,
                                                                                              data_split_name=args.data_split,
                                                                                              target_file_or_dir=target,
                                                                                              subspace_name=args.subspace,
                                                                                              workspace_root_path='',
                                                                                              abs_path=False,
                                                                                              unpack_for_single_result=False,
                                                                                              return_first_only=return_first_only)))
    else:
        return argex.apply_arg_combo(get_workspace_path,
                                     data_type=data_type,
                                     data_set_name=args.data_set,
                                     data_split_name=args.data_split,
                                     target_file_or_dir=target,
                                     subspace_name=args.subspace,
                                     workspace_root_path=args.workspace_root,
                                     unpack_for_single_result=unpack_for_single_result,
                                     return_first_only=return_first_only)


def get_output_path(args, data_type: str, target: str = '', connector='-', **kwargs):
    args = argex.update_args(args, **kwargs)
    if not isinstance(args.data_set, str):
        args.data_set = '-'.join(args.data_set)
    if not isinstance(args.data_split, str):
        args.data_set = '-'.join(args.data_split)
    return get_workspace_path(data_type=data_type,
                              data_set_name=args.data_set,
                              data_split_name=args.data_split,
                              target_file_or_dir=target,
                              subspace_name=args.subspace,
                              workspace_root_path=args.workspace_root)


def get_cache(args) -> Cache:
    if hasattr(args, 'cache_path') and not getattr(args, 'no_file_cache', False):
        return SimpleFileCache(cache_dir=args.cache_path,
                               compressed=args.compress_cache,
                               shuffle_file_order=args.shuffle_cache)


class ExperimentBase__:
    def __init__(self, *arg_info_objs, arg_preset_root: str = None, arg_preset: [dict, str] = None, experiment_name_args=('mode',), experiment_name: str = None, experiment_name_prefix: str = None, experiment_name_suffix: str = None, **kwargs):
        for k, v in kwargs.items():
            if k.startswith('dir_') and v is not None:
                dir_value_key = k
                get_input_path_method_key = f'input_path_from_{k[4:]}'
                get_output_path_method_key = f'output_path_to_{k[4:]}'
                msgex.assert_name_not_defined(obj=self, member_name=dir_value_key)
                # ! no longer make the name existence checks; we need to declare them for IDE auto-completion
                # msgex.assert_name_not_defined(obj=self, member_name=get_input_path_method_key)
                # msgex.assert_name_not_defined(obj=self, member_name=get_output_path_method_key)
                setattr(self, dir_value_key, v)
                setattr(self, get_input_path_method_key, partial(lambda data_type, **kwargs2: self._get_input_path(data_type=getattr(self, data_type), **kwargs2), data_type=dir_value_key))
                setattr(self, get_output_path_method_key, partial(lambda data_type, **kwargs3: self._get_output_path(data_type=getattr(self, data_type), **kwargs3), data_type=dir_value_key))

        self.args, self.terminal_set_arg_names = argex.get_parsed_args(*((('workspace_root', '.', 'The root path to the experiment workspace. Multiple workspace roots are possibly OK but discouraged and not supported.'),
                                                                          ('subspace', '', 'The name of the sub-workspace to use. Multiple sub-workspaces are possibly OK but discouraged and not supported.'),
                                                                          ('data_set', 'main_data', 'The name(s) of the data set(s) to use in the experiment; multiple data sets are supported.'),
                                                                          ('data_split', 'train', 'The name(s) for the experiment data split(s), e.g. the training data for modeling-based experiments; multiple data splits are supported.'),
                                                                          ('mode', '', 'The experiment mode. Interpretation of this parameter is dependent on the specific experiment.'),) + arg_info_objs), preset_root=arg_preset_root, preset=arg_preset, return_seen_args=True, **kwargs)

        self.name = f'_'.join((x for x in (experiment_name_prefix, experiment_name, experiment_name_suffix) if x is not None)) if experiment_name \
            else argex.args2str(args=self.args,
                                active_arg_name_info_tuples=experiment_name_args,
                                name_val_delimiter='',
                                prefix=experiment_name_prefix,
                                suffix=experiment_name_suffix)

    def _get_input_path(self, data_type, target='', **kwargs):
        return get_input_path(args=self.args, data_type=data_type, target=target, **kwargs)

    def _get_output_path(self, data_type, target='', **kwargs):
        return get_output_path(args=self.args, data_type=data_type, target=target, **kwargs)

    def unprefix_args(self, prefix):
        len_prefix = len(prefix)
        return {k[len_prefix:]: v for k, v in self.args.__dict__.items() if k.startswith(prefix)}

    def get_arg_names(self, prefix):
        return tuple(k for k in self.args.__dict__ if k.startswith(prefix))


class ExperimentBase(ExperimentBase__, msgex.LoggableBase):
    # region common path functions
    def meta_data_path(self, target='', output=False, **kwargs):
        return self.output_path_to_meta_data(target=target, **kwargs) if output else self.input_path_from_meta_data(target=target, **kwargs)

    def source_data_path(self, target='', output=False, **kwargs):
        return self.output_path_to_source_data(target=target, **kwargs) if output else self.input_path_from_source_data(target=target, **kwargs)

    def data_sets_path(self, target='', output=False, **kwargs):
        return self.output_path_to_data_sets(target=target, **kwargs) if output else self.input_path_from_data_sets(target=target, **kwargs)

    def analysis_path(self, target='', output=False, **kwargs):
        return self.output_path_to_analysis(target=target, **kwargs) if output else self.input_path_from_analysis(target=target, **kwargs)

    def results_path(self, target='', output=False, **kwargs):
        return self.output_path_to_results(target=target, **kwargs) if output else self.input_path_from_results(target=target, **kwargs)

    def input_path_from_meta_data(self, target='', **kwargs):
        ...

    def input_path_from_source_data(self, target='', **kwargs):
        ...

    def input_path_from_data_sets(self, target='', **kwargs):
        ...

    def input_path_from_analysis(self, target='', **kwargs):
        ...

    def input_path_from_results(self, target='', **kwargs):
        ...

    def output_path_to_meta_data(self, target='', **kwargs):
        ...

    def output_path_to_source_data(self, target='', **kwargs):
        ...

    def output_path_to_data_sets(self, target='', **kwargs):
        ...

    def output_path_to_analysis(self, target='', **kwargs):
        ...

    def output_path_to_results(self, target='', **kwargs):
        ...

    # endregion

    def __init__(self,
                 *arg_info_objs,
                 dir_meta_data='meta_data',
                 dir_source_data='source_data',
                 dir_datasets='datasets',
                 dir_results='results',
                 dir_analysis='analysis',
                 arg_preset_root: str = None,
                 arg_preset: [dict, str] = None,
                 default_workspace_root='.',
                 general_args=True,
                 workspace_override_args=True,
                 deep_learning_args=True,
                 nlp_args=True,
                 simple_args=False,
                 experiment_name_args=(),
                 experiment_name_prefix='',
                 experiment_name_suffix='',
                 **kwargs):
        """
        A base class for workspace and configuration management.

        Workspace.
        ----------
        This class automatically creates directories under the workspace root folder, and provides methods to access the specified data folders or files.
        We recommend five workspace data types, and 6-level workspace structure.

        The five workspace data types include:
        1) **meta data**: store the data shared by all experiments.
        2) **source data**: store the initial data; the source data is typically not cleaned or filtered and contains extra information.
        3) **datasets**: save processed data directly used for the experiments; can hold data splits like the training data, validation data and test data.
        4) **results**: save the experiment results, typically the results of the main experiment pipeline.
        5) **analysis**: save data exploration results, further analysis on the experiment results or other miscellaneous results; an analysis is typically done by quick scripts that explore the data or main experiment results.

        The six-level workspace structure are: 1) workspace root -> 2) sub workspace (optional) -> 3) data type -> 4) data set -> 5) data split -> 6) file or dir (optional).
        See the `get_workspace_path` function for details.

        Argument Parsing.
        -----------------
        This class parses the terminal arguments using `utilx.argex.get_parsed_args` function.

        :param arg_info_objs: argument definition objects; this is passed to `utilx.argex.get_parsed_args` for parsing terminal arguments.
        :param dir_meta_data: the directory name for the metadata; specify `None` to indicate there is no need for a metadata directory.
        :param dir_source_data: the directory name for the source data; specify `None` to indicate there is no need for a source-data directory.
        :param dir_datasets: the directory name for the datasets; specify `None` to indicate there is no need for a dataset directory;
        :param dir_results: the directory name for the experiment results; specify `None` to indicate there is no need for a result directory;
        :param dir_analysis: the directory name for the data and result analysis; specify `None` to indicate there is no need for an analysis directory;
        :param arg_preset_root: the path to the directory that stores presets of arguments; this is passed to the `utilx.argex.get_parsed_args` function.
        :param arg_preset: the path/name of the argument preset file relative to `arg_preset_root`; this is passed to the `utilx.argex.get_parsed_args` function.
        :param default_workspace_root: the path to the default workspace root folder.
        :param general_args: `True` to add build-in common argument definitions to the `arg_info_objs`.
        :param workspace_override_args: `True` to add build-in specialized workspace argument definitions to the `arg_info_objs`.
        :param deep_learning_args: `True` to add build-in deep-learning argument definitions to the `arg_info_objs`.
        :param nlp_args: `True` to add build-in NLP argument definitions to the `arg_info_objs`.
        :param simple_args: a convenient argument; set this to `True` if the experiment is a simple script that only uses specified arguments in the `arg_info_objs`; otherwise, `False`.
        :param experiment_name_args: specify the names of important arguments, and these argument names and values are used to construct the experiment name so that the experiment name provides hint to what arguments are used for the experiment.
        :param experiment_name_prefix: the prefix for the experiment name.
        :param experiment_name_suffix: the suffix for the experiment name.
        :param kwargs: other arguments.
        """
        arg_info_objs += ARG_SETUP_ESSENTIAL
        if not simple_args:
            arg_info_objs += ((ARG_SETUP_GENERAL if general_args else ()) +
                              (ARG_WORKSPACE_OVERRIDES if workspace_override_args else ()) +
                              (ARG_SETUP_DEEP_LEARNING_GENERAL if deep_learning_args else ()) +
                              (ARG_SETUP_NLP if nlp_args else ()))
            experiment_name_args += (EXP_NAME_ARGS_GENERAL if general_args else ()) + (EXP_NAME_ARGS_DEEP_LEARNING_GENERAL if deep_learning_args else ())
        ExperimentBase__.__init__(self,
                                  *arg_info_objs,
                                  dir_meta_data=dir_meta_data,
                                  dir_source_data=dir_source_data,
                                  dir_data_sets=dir_datasets,
                                  dir_analysis=dir_analysis,
                                  dir_results=dir_results,
                                  arg_preset_root=arg_preset_root,
                                  arg_preset=arg_preset,
                                  default_workspace_root=default_workspace_root,
                                  experiment_name_args=experiment_name_args,
                                  experiment_name_prefix=experiment_name_prefix,
                                  experiment_name_suffix=experiment_name_suffix,
                                  **kwargs)

        # region adds convenient fields to `args`
        args = self.args
        if not simple_args:
            setattr_if_none_or_empty__(args, 'result_path', lambda: self.output_path_to_results(target=self.name))
            setattr_if_none_or_empty__(args, 'runtime_path', lambda: paex.path_or_name_with_timestamp(path.join(args.result_path, 'run')))
            setattr_if_none_or_empty__(args, 'log_path', lambda: path.join(args.runtime_path, '_logs'))
            msgex.LoggableBase.__init__(self, path.join(args.log_path, 'main.log'), print_out=kwargs.get('verbose'))
            self.info_message('experiment result path', args.result_path)
            self.info_message('experiment runtime path', args.runtime_path)
            self.info_message('experiment log path', args.log_path)
            setattr_if_none_or_empty__(args, 'cache_path', lambda: path.join(args.result_path, '_cache'))
            self.info_message('experiment cache path', args.cache_path)
            setattr_if_none_or_empty__(args, 'data_path', lambda: self.input_path_from_data_sets())
            self.info_message('experiment data path', args.data_path)

            if general_args:
                setattr_if_none_or_empty__(args, 'test_set_path', lambda: self.input_path_from_test_sets())
                self.info_message('experiment test set path', args.test_set_path)
                setattr_if_none_or_empty__(args, 'val_set_path', lambda: self.input_path_from_val_sets())
                self.info_message('experiment validation set path', args.val_set_path)

        # endregion

    def get_logger(self, log_name: str, logging_level=logging.DEBUG, log_format="%(asctime)s - %(process)d - %(name)s - %(levelname)s - %(message)s", append=False, file_ext='log'):
        if hasattr(self.args, 'log_path'):
            return msgex.get_logger(name=log_name, log_dir_path=self.args.log_path, logging_level=logging_level, log_format=log_format, append=append, file_ext=file_ext)

    def get_cache(self) -> Cache:
        return get_cache(self.args)

    def output_path_to_default_log_file(self):
        return self.output_path_to_logs(target=f'{self.name}.log')

    def input_path_from_test_sets(self, target: Union[str, Iterable] = '', multi_path=False, **kwargs) -> str:
        return get_input_path(self.args, data_type=self.dir_data_sets, data_split=self.args.test_split, target=target, multipath=multi_path, **kwargs) if self.args.test_split else None

    def input_path_from_val_sets(self, target: Union[str, Iterable] = '', multi_path=False, **kwargs) -> str:
        return get_input_path(self.args, data_type=self.dir_data_sets, data_split=self.args.val_split, target=target, multipath=multi_path, **kwargs) if self.args.val_split else None


def SimpleExperiment(*arg_info_objs,
                     dir_meta_data='meta_data',
                     dir_source_data='source_data',
                     dir_data_sets='data_sets',
                     dir_analysis='analysis',
                     dir_results='results',
                     default_arg_preset_root: str = None,
                     arg_preset: Union[str, dict] = None,
                     default_workspace_root='.',
                     workspace_override_args=True,
                     verbose=False, **kwargs):
    return ExperimentBase(*arg_info_objs,
                          dir_meta_data=dir_meta_data,
                          dir_source_data=dir_source_data,
                          dir_datasets=dir_data_sets,
                          dir_analysis=dir_analysis,
                          dir_results=dir_results,
                          arg_preset_root=default_arg_preset_root,
                          arg_preset=arg_preset,
                          default_workspace_root=default_workspace_root,
                          general_args=False,
                          workspace_override_args=workspace_override_args,
                          deep_learning_args=False,
                          nlp_args=False,
                          verbose=verbose,
                          **kwargs)


_ARG_DEFAULT_TEST_SPLIT_NAME = 'test'
_ARG_DEFAULT_VAL_SPLIT_NAME = 'val'
_ARG_DEFAULT_DATA_MAX_READ = 100000
_ARG_DEFAULT_MULTIPROCESSING = 1
_ARG_DEFAULT_VERBOSE = True
_ARG_DEFAULT_RAND_SEED = 0
_ARG_DEFAULT_VAL_PROB = 0.1
_ARG_DEFAULT_VAL_MAX_SIZE = 5000
_ARG_DEFAULT_TEST_PROB = 0.1
_ARG_DEFAULT_TEST_MAX_SIZE = 5000
_ARG_DEFAULT_NO_FILE_CACHE = False
_ARG_DEFAULT_NO_MEMORY_CACHE = False
_ARG_DEFAULT_COMPRESS_CACHE = False
_ARG_DEFAULT_SHUFFLE_CACHE = False
_ARG_DEFAULT_LESS_EVAL_WHILE_TRAINING = True
_ARG_DEFAULT_MAX_ITER = 0
_ARG_DEFAULT_PRE_TEST_INTERVAL = 0
_ARG_DEFAULT_STAT_HARDWARE_USAGE = False
ARG_SETUP_ESSENTIAL = (
    ('verbose', _ARG_DEFAULT_VERBOSE, 'Printout/Log internal messages when applicable.'),
    ('random_seed', _ARG_DEFAULT_RAND_SEED, 'The random seed for reproducible experiments.')
)
ARG_SETUP_GENERAL = (
    ('test_split', _ARG_DEFAULT_TEST_SPLIT_NAME, 'The name(s) for the test data split(s); multiple data splits are supported.'),
    ('val_split', _ARG_DEFAULT_VAL_SPLIT_NAME, 'The name(s) for the validation data split(s); multiple data splits are supported.'),
    ('data_max_read', _ARG_DEFAULT_DATA_MAX_READ, 'Limit the maximum number of loaded data entries for experiment.'),
    ('multi_processing', _ARG_DEFAULT_MULTIPROCESSING, 'The number of parallel processes for parallel processing when applicable.'),
    ('test_ratio', _ARG_DEFAULT_TEST_PROB, 'The sampling ratio (probability) for the test set.'),
    ('val_ratio', _ARG_DEFAULT_VAL_PROB, 'The sampling ratio (probability) for the validation set.'),
    ('test_max_size', _ARG_DEFAULT_TEST_MAX_SIZE, 'The maximum size of the test set.'),
    ('val_max_size', _ARG_DEFAULT_VAL_MAX_SIZE, 'The maximum size of the validation set.'),
    ('model_init', '', 'The model initialization; interpretation dependent on the experiment.'),
    ('no_file_cache', _ARG_DEFAULT_NO_FILE_CACHE, 'Disables any file-based caching mechanism when applicable.'),
    ('no_mem_cache', _ARG_DEFAULT_NO_MEMORY_CACHE, 'Disables any memory-based caching mechanism when applicable.'),
    ('compress_cache', _ARG_DEFAULT_COMPRESS_CACHE, 'Compressed the cached data when applicable.'),
    ('shuffle_cache', _ARG_DEFAULT_SHUFFLE_CACHE, 'The cached data items are shuffled when applicable.'),
    ('less_eval_while_training', _ARG_DEFAULT_LESS_EVAL_WHILE_TRAINING, 'Serves as a signal to the experiment pipeline to fields_sub the cost of evaluation during training when applicable (e.g. no detailed model output dump).'),
    ('pre_test_interval', _ARG_DEFAULT_PRE_TEST_INTERVAL, 'Sets a positive integer to enable premature test during training (i.e. evaluate on the test set before fully training the model for all epochs); '
                                                          'the integer is the number of epochs between two tests. '
                                                          'Set this number to 0 to disable premature tests.'),
    ('max_iter', _ARG_DEFAULT_MAX_ITER, 'The maximum number of iterations. The interpretation of this number is model/experiment dependent.'),
    ('stat_hardware_usage', _ARG_DEFAULT_STAT_HARDWARE_USAGE, 'The maximum number of iterations. The interpretation of this number is model/experiment dependent.')
)
EXP_NAME_ARGS_GENERAL = ('random_seed', 'data_max_read')

ARG_WORKSPACE_OVERRIDES = (
    ('result_path', None, 'The path to the result directory. Specify this to override the workspace result path.'),
    ('runtime_path', None, 'The path to the current runtime directory. Specify this to override the workspace runtime path.'),
    ('log_path', None, 'The path to the log directory. Specify this to override the workspace log path.'),
    ('cache_path', None, 'The path to the cache directory. Specify this to override the workspace cache path.'),
    ('data_path', None, 'The path to the cache directory. Specify this to override the workspace data path.'),
    ('test_set_path', None, 'The path to the test set directory. Specify this to override the workspace test set path.'),
    ('val_set_path', None, 'The path to the validation set directory. Specify this to override the workspace validation set path.')
)

_ARG_DEFAULT_GPU_INDEX = 0
_ARG_DEFAULT_BATCH_SIZE = 64
_ARG_DEFAULT_BATCH_CACHE_GROUP_SIZE = 64
_ARG_DEFAULT_TEST_BATCH_SIZE = -1
_ARG_DEFAULT_EPOCHS = 20
_ARG_DEFAULT_LEARNING_RATE = 0.001
_ARG_DEFAULT_REG_LAMBDA = 1E-4
_ARG_DEFAULT_MODEL_TEST_TOP_K = 3
_ARG_DEFAULT_WEIGHT_DECAY = 5E-4
_ARG_DEFAULT_DROPOUT = 0
_ARG_DEFAULT_EARLY_STOP_PATIENCE = 10
_ARG_DEFAULT_INPUT_STATE_DIM = 300
_ARG_DEFAULT_HIDDEN_STATE_DIM = [512, 256, 128]
_ARG_DEFAULT_MAX_SEQ_LEN = 18
_ARG_DEFAULT_SIMILARITY_LAYER = 'dot'
_ARG_DEFAULT_POOL_LAYER = 'masked-max'
_ARG_DEFAULT_EMBEDDING_SAVE_OPTION = 'numpy'
_ARG_DEFAULT_OUTPUT_SAVE_RATIO = 0.0
_ARG_DEFAULT_MODEL_SAVE_RATIO = 0.5
_ARG_DEFAULT_MODEL_SAVE_MIN = 3
ARG_SETUP_DEEP_LEARNING_GENERAL = (
    ('gpu', _ARG_DEFAULT_GPU_INDEX, 'The index of the GPU to use.'),
    ('batchsize', _ARG_DEFAULT_BATCH_SIZE, 'The training batch size.'),
    ('test_batchsize', _ARG_DEFAULT_BATCH_SIZE, 'The test batch size.'),
    ('epochs', _ARG_DEFAULT_EPOCHS, 'The number of training epochs'),
    ('learning_rate', _ARG_DEFAULT_LEARNING_RATE, 'The learning rate. Rule of thumb: higher rate for simpler models; lower rate for complex models.'),
    ('reg_lambda', _ARG_DEFAULT_REG_LAMBDA, 'The regularization penalty.'),
    ('weight_decay', _ARG_DEFAULT_WEIGHT_DECAY, 'The optimization weight decay.'),
    ('dropout', _ARG_DEFAULT_DROPOUT, 'The optimization weight decay.'),
    ('early_stop_patience', _ARG_DEFAULT_EARLY_STOP_PATIENCE, 'During training, if the average performance declines for this number of previous epochs, '
                                                              'then the early-stop is activated to terminate the entire training process without finishing the remaining epochs; '
                                                              'to disable early-stop, assign 0 to this parameter'),
    ('model_test_top_k', _ARG_DEFAULT_MODEL_TEST_TOP_K, 'Test the top-k models on the test sets.'),
    ('input_state_dim', _ARG_DEFAULT_INPUT_STATE_DIM, 'The dimension of the input states (embedding).'),
    ('hidden_state_dim', _ARG_DEFAULT_HIDDEN_STATE_DIM, 'The dimension of the hidden states (embeddings). May specify a list of integers for multiple layers of hidden state dimensions.'),
    ('max_seq_len', _ARG_DEFAULT_MAX_SEQ_LEN, 'The maximum sequence length needed for sequence models.'),
    ('similarity_layer', _ARG_DEFAULT_SIMILARITY_LAYER, 'The similarity function used in the neural network when applicable. A similarity layer is common in information retrieval models.'),
    ('pool_layer', _ARG_DEFAULT_POOL_LAYER, 'The pooling layer used in the neural network when applicable. A pooling layer is common in NLP and computer vision models.'),
    ('batch_cache_group_size', _ARG_DEFAULT_BATCH_CACHE_GROUP_SIZE, 'The number of batches in one cache group; (e.g. for a file cache, a cache group is typically written in a cache file).'),
    ('embedding_save_option', _ARG_DEFAULT_EMBEDDING_SAVE_OPTION, 'The options for embedding save. Currently support: 1) original, the embedding is saved as it is; 2) cpu, the embedding is saved as CPU tensors; 3) numpy, the embedding is saved as numpy arrays.'),
    ('output_save_ratio', _ARG_DEFAULT_OUTPUT_SAVE_RATIO, 'Specifies the ratio of outputs to save when applying the model on the input data. The primary purpose of this parameter is to extract a percentage of model output for visualization or analysis.'),
    ('model_save_ratio', _ARG_DEFAULT_OUTPUT_SAVE_RATIO, 'Specifies the ratio of models to save during the training. Typically, this ratio is with respect to the number of epochs. For example, when training for 10 epochs, and this ratio is 0.5, then we save models for the last 5 epochs.'),
    ('model_save_min', _ARG_DEFAULT_MODEL_SAVE_MIN, 'Specifies the minimum number of models to save during the training. Use this parameter together with `model_save_ratio` to determine the actual number of saved models.')
)
EXP_NAME_ARGS_DEEP_LEARNING_GENERAL = ('batchsize', 'epochs', 'learning_rate', 'dropout')
ARG_SETUP_NLP = ()


class ExpUnitData:
    __slots__ = ('data_reader', 'iterator')


class ExpUnitModel:
    __slots__ = ('model', 'input_func', 'loss_func', 'optimizer', 'output_func')


class ExpUnitEvaluation:
    __slots__ = ('metrics', 'metric_func', 'analyzers')


class ExpUnitNLP:
    __slots__ = ('tokenizer_indexers', 'vocab', 'to_instance')
