import logging
from collections import namedtuple
from functools import partial
from os import path
from typing import Union, Iterable

import utix.argex as argex
import utix.msgex as msgex
import utix.pathex as paex
import json
from sys import argv
from utix.nlpu import Vocabulary
from utix.general import setattr_if_none_or_empty__, SlotsTuple
from utix.ioex import SimpleFileCache, Cache, write_json


class ExpArgInfo(SlotsTuple):
    """
    Namedtuple for experiment argument definition.
    """
    __slots__ = argex.ArgInfo.__slots__ + ('affect_batch', 'affect_vocab', 'could_be_path')

    def __init__(self, full_name='', short_name='', default_value=None, description='', converter=None, affect_batch=False, affect_vocab=False, could_be_path=False):
        self.full_name = full_name
        self.short_name = short_name
        self.default_value = default_value
        self.description = description
        self.converter = converter
        self.affect_batch = affect_batch
        self.affect_vocab = affect_vocab
        self.could_be_path = could_be_path
        super(ExpArgInfo, self).__init__()


def _arg_backward_compatibility_fix(args):
    name_map = (
        ('similarity', 'similarity_layer'),
        ('pool', 'pool_layer'),
        ('batch_size', 'batchsize'),
        ('test_batch_size', 'test_batchsize'),
        ('initialization', 'model_init'),
        ('no_cache', 'no_file_cache'),
        ('no_cache_instances', 'no_mem_cache'),
        ('cache_shuffle', 'shuffle_cache'),
        ('cache_compressed', 'compress_cache'),
        ('eval_when_training', 'less_eval_while_training')
    )


def get_workspace_path(data_type: str,
                       dataset_name: str,
                       data_split_name: str,
                       target_file_or_dir: Union[str, None] = None,
                       workspace_root_path: str = '.',
                       subspace_name: Union[str, None] = None,
                       abs_path=True):
    """
    Defines a 6-level workspace structure. From top-down, they are
    1) workspace root -> 2) sub workspace (optional) -> 3) data type -> 4) data set -> 5) data split -> 6) file or dir (optional).

    The **workspace root** is the root dir of the entire workspace, whose path is specified by `workspace_root_path`;
    The **sub workspace** is an optional subdir immediately under the workspace root, whose name is specified by `sub_workspace_name`;
        a typical use of sub workspaces is to serve as the workspaces for the same experiment over different versions of data;
    The **data type** is a subdir immediately under a workspace (the workspace root or a sub workspace), separating different types of experiment data;
        we recommend six types 'meta_data', 'source_data', 'datasets', 'results', 'analysis', 'logs';
        'meta_data': data shared by all experiments;
        'source_data': the original data from which the data sets can be built
        'datasets': the data after pre-processing, the data which the experiment code can directly apply on;
        'results': the experiment results;
        'analysis': any analysis results, e.g. analysis for the source data, or analysis for the results, or plot, etc;
        'logs': experiment code logs generated by the logging modules.
    The **data set** is a subdir immediately under the 'data-type' level dir, which saves data of the particular type related to a data set.
    The **data split** is a subdir immediately under a 'data-set' level dir, which saves a split of the data set;
        typical data splits are train/test/validation splits;
        for data with timestamps, the data splits can also be months, so that we train a model with previous months and test on the next few months.
    Finally, the optional **target file or directory**, the name of the actually file or directory to look for; if this is not specified, then we look for the path to the data-split level directory.

    :param data_type: the data-type level directory name; e.g. 'meta_data', 'source_data', 'datasets', 'results', 'analysis', 'logs', etc.
    :param dataset_name: the data set (directory) name.
    :param data_split_name: the data split (directory) name; e.g. 'train', 'test', 'val', etc.
    :param target_file_or_dir: the name of the target file or directory under the data split directory; if looking for the path the the data-split level directory, then leave this parameter as `None`.
    :param workspace_root_path: the path to the workspace dir.
    :param subspace_name: the name for the sub-workspace; if a sub-workspace is not needed, leave this parameter as `None`.
    :param abs_path: `True` if an absolute path should always be returned; otherwise `False`.
    :return: the path to the target file or directory.
    """

    msgex.ensure_arg_not_none(arg_val=workspace_root_path, arg_name='workspace_root', extra_msg='workspace root is not provided')
    msgex.ensure_arg_not_none_or_empty(arg_val=data_type, arg_name='data_type', extra_msg='data type is not specified')
    path_str = path.join(*(x for x in (workspace_root_path, subspace_name, data_type, dataset_name, data_split_name, target_file_or_dir) if x))
    return paex.abspath__(path_str) if abs_path else path_str


def get_input_path(args, data_type: str, target: Union[str, Iterable] = '', multipath=False, multi_path_delimiter=paex.DEFAULT_MULTI_PATH_DELIMITER, unpack_for_single_result=True, return_first_only=False, **kwargs):
    """
    Gets path(s) from certain types of data in the workspace.
    This method assumes the workspace is organized with four levels: data_type/dataset/data_split/target.
    :param args:
    :param data_type:
    :param target:
    :param multipath: `True` if this method returns a single string as a 'multi-path'.
    :param multi_path_delimiter:
    :param unpack_for_single_result:
    :param return_first_only:
    :param kwargs:
    :return:
    """
    args = argex.update_args(args, **kwargs)
    if multipath:
        return path.join(args.workspace_root, multi_path_delimiter.join(argex.apply_arg_combo(get_workspace_path,
                                                                                              data_type=data_type,
                                                                                              dataset_name=args.dataset,
                                                                                              data_split_name=args.data_split,
                                                                                              target_file_or_dir=target,
                                                                                              subspace_name=args.subspace,
                                                                                              workspace_root_path='',
                                                                                              abs_path=False,
                                                                                              unpack_for_single_result=False,
                                                                                              return_first_only=return_first_only)))
    else:
        return argex.apply_arg_combo(get_workspace_path,
                                     data_type=data_type,
                                     dataset_name=args.dataset,
                                     data_split_name=args.data_split,
                                     target_file_or_dir=target,
                                     subspace_name=args.subspace,
                                     workspace_root_path=args.workspace_root,
                                     unpack_for_single_result=unpack_for_single_result,
                                     return_first_only=return_first_only)


def get_output_path(args, data_type: str, target: str = '', connector='-', **kwargs):
    args = argex.update_args(args, **kwargs)
    if not isinstance(args.dataset, str):
        args.dataset = connector.join(args.dataset)
    if not isinstance(args.data_split, str):
        args.dataset = connector.join(args.data_split)
    return paex.ensure_parent_dir_existence(get_workspace_path(data_type=data_type,
                                                               dataset_name=args.dataset,
                                                               data_split_name=args.data_split,
                                                               target_file_or_dir=target,
                                                               subspace_name=args.subspace,
                                                               workspace_root_path=args.workspace_root))


def get_cache(args) -> Cache:
    if hasattr(args, 'cache_path') and not getattr(args, 'no_file_cache', False):
        return SimpleFileCache(cache_dir=args.cache_path,
                               compressed=args.compress_cache,
                               shuffle_file_order=args.shuffle_cache)


class ExperimentBase__:
    def __init__(self, *arg_info_objs, preset_root: str = None, preset: [dict, str] = None,
                 expname_args=('mode',), expname: str = None, expname_prefix: str = None, expname_suffix: str = None,
                 batchname_args=('batchsize',), batchname: str = None, batchname_prefix: str = None, batchname_suffix: str = None,
                 **kwargs):
        for k, v in kwargs.items():
            if k.startswith('dir_') and v is not None:
                dir_value_key = k
                get_input_path_method_key = f'input_path_from_{k[4:]}'
                get_output_path_method_key = f'output_path_to_{k[4:]}'
                msgex.assert_name_not_defined(obj=self, member_name=dir_value_key)
                # ! no longer make the name existence checks; we need to declare them for IDE auto-completion
                # msgex.assert_name_not_defined(obj=self, member_name=get_input_path_method_key)
                # msgex.assert_name_not_defined(obj=self, member_name=get_output_path_method_key)
                setattr(self, dir_value_key, v)
                setattr(self, get_input_path_method_key, partial(self._get_input_path, data_type=getattr(self, dir_value_key)))
                setattr(self, get_output_path_method_key, partial(self._get_output_path, data_type=getattr(self, dir_value_key)))

        self.args, self.terminal_set_arg_names = argex.get_parsed_args(*((('workspace_root', '.', 'The root path to the experiment workspace. Multiple workspace roots are possibly OK but discouraged and not supported.'),
                                                                          ('subspace', '', 'The name of the sub-workspace to use. Multiple sub-workspaces are possibly OK but discouraged and not supported.'),
                                                                          ('dataset', 'main_data', 'The name(s) of the data set(s) to use in the experiment; multiple data sets are supported.'),
                                                                          ('data_split', 'train', 'The name(s) for the experiment data split(s), e.g. the training data for modeling-based experiments; multiple data splits are supported.'),
                                                                          ('mode', 'default', 'The experiment mode. Interpretation of this parameter is dependent on the specific experiment.'),) + arg_info_objs), preset_root=preset_root, preset=preset, return_seen_args=True, **kwargs)

        self.name = argex.getname(args=self.args,
                                  active_argname_info=expname_args,
                                  name=expname,
                                  name_prefix=expname_prefix,
                                  name_suffix=expname_suffix)
        self.batchname = argex.getname(args=self.args,
                                       active_argname_info=batchname_args,
                                       name=batchname,
                                       name_prefix=batchname_prefix,
                                       name_suffix=batchname_suffix)

    def _get_input_path(self, target='', data_type='', **kwargs):
        return get_input_path(args=self.args, data_type=data_type, target=target, **kwargs)

    def _get_output_path(self, target='', data_type='', **kwargs):
        return get_output_path(args=self.args, data_type=data_type, target=target, **kwargs)

    def unprefix_args(self, prefix):
        len_prefix = len(prefix)
        return {k[len_prefix:]: v for k, v in self.args.__dict__.items() if k.startswith(prefix)}

    def get_arg_names(self, prefix):
        return tuple(k for k in self.args.__dict__ if k.startswith(prefix))


class ExperimentBase(ExperimentBase__, msgex.LoggableBase):
    # region common path functions
    def meta_data_path(self, target='', output=False, **kwargs):
        return self.output_path_to_meta_data(target=target, **kwargs) if output else self.input_path_from_meta_data(target=target, **kwargs)

    def source_data_path(self, target='', output=False, **kwargs):
        """
        Gets the path to the particular source data specified by `target`.
        :param target:
        :param output:
        :param kwargs:
        :return:
        """
        return self.output_path_to_source_data(target=target, **kwargs) if output else self.input_path_from_source_data(target=target, **kwargs)

    def datasets_path(self, target='', output=False, **kwargs):
        return self.output_path_to_datasets(target=target, **kwargs) if output else self.input_path_from_datasets(target=target, **kwargs)

    def analyscould_be_path(self, target='', output=False, **kwargs):
        return self.output_path_to_analysis(target=target, **kwargs) if output else self.input_path_from_analysis(target=target, **kwargs)

    def results_path(self, target='', output=False, **kwargs):
        return self.output_path_to_results(target=target, **kwargs) if output else self.input_path_from_results(target=target, **kwargs)

    def input_path_from_meta_data(self, target='', **kwargs):
        ...

    def input_path_from_source_data(self, target='', **kwargs):
        ...

    def input_path_from_datasets(self, target='', **kwargs):
        ...

    def input_path_from_analysis(self, target='', **kwargs):
        ...

    def input_path_from_results(self, target='', **kwargs):
        ...

    def output_path_to_meta_data(self, target='', **kwargs) -> str:
        ...

    def output_path_to_source_data(self, target='', **kwargs) -> str:
        ...

    def output_path_to_datasets(self, target='', **kwargs) -> str:
        ...

    def output_path_to_analysis(self, target='', **kwargs) -> str:
        ...

    def output_path_to_results(self, target='', **kwargs) -> str:
        ...

    # endregion

    def __init__(self,
                 *arg_info_objs,
                 dir_meta_data='meta_data',
                 dir_source_data='source_data',
                 dir_datasets='datasets',
                 dir_results='results',
                 dir_analysis='analysis',
                 preset_root: str = None,
                 preset: [dict, str] = None,
                 default_workspace_root='.',
                 general_args=True,
                 workspace_override_args=True,
                 deep_learning_args=True,
                 nlp_args=True,
                 simple_args=False,
                 expname_args=(),
                 expname_prefix='',
                 expname_suffix='',
                 batchname_args=(),
                 batchname_prefix='',
                 batchname_suffix='',
                 **kwargs):
        """
        A base class for workspace and configuration management.

        Workspace.
        ----------
        This class automatically creates directories under the workspace root folder, and provides methods to access the specified data folders or files.
        We recommend five workspace data types, and 6-level workspace structure.

        The five workspace data types include:
        1) **meta data**: store the data shared by all experiments.
        2) **source data**: store the initial data; the source data is typically not cleaned or filtered and contains extra information.
        3) **datasets**: save processed data directly used for the experiments; can hold data splits like the training data, validation data and test data.
        4) **results**: save the experiment results, typically the results of the main experiment pipeline.
        5) **analysis**: save data exploration results, further analysis on the experiment results or other miscellaneous results; an analysis is typically done by quick scripts that explore the data or main experiment results.

        The six-level workspace structure are: 1) workspace root -> 2) sub workspace (optional) -> 3) data type -> 4) data set -> 5) data split -> 6) file or dir (optional).
        See the `get_workspace_path` function for details.

        Argument Parsing.
        -----------------
        This class parses the terminal arguments using `utix.argex.get_parsed_args` function.

        :param arg_info_objs: argument definition objects; this is passed to `utix.argex.get_parsed_args` for parsing terminal arguments.
        :param dir_meta_data: the directory name for the metadata; specify `None` to indicate there is no need for a metadata directory.
        :param dir_source_data: the directory name for the source data; specify `None` to indicate there is no need for a source-data directory.
        :param dir_datasets: the directory name for the datasets; specify `None` to indicate there is no need for a dataset directory;
        :param dir_results: the directory name for the experiment results; specify `None` to indicate there is no need for a result directory;
        :param dir_analysis: the directory name for the data and result analysis; specify `None` to indicate there is no need for an analysis directory;
        :param preset_root: the path to the directory that stores presets of arguments; this is passed to the `utix.argex.get_parsed_args` function.
        :param preset: the path/name of the argument preset file relative to `arg_preset_root`; this is passed to the `utix.argex.get_parsed_args` function.
        :param default_workspace_root: the path to the default workspace root folder.
        :param general_args: `True` to add build-in common argument definitions to the `arg_info_objs`.
        :param workspace_override_args: `True` to add build-in specialized workspace argument definitions to the `arg_info_objs`.
        :param deep_learning_args: `True` to add build-in deep-learning argument definitions to the `arg_info_objs`.
        :param nlp_args: `True` to add build-in NLP argument definitions to the `arg_info_objs`.
        :param simple_args: a convenient argument; set this to `True` if the experiment is a simple script that only uses specified arguments in the `arg_info_objs`; otherwise, `False`.
        :param expname_args: specify the names of important arguments, and these argument names and values are used to construct the experiment name so that the experiment name provides hint to what arguments are used for the experiment.
        :param expname_prefix: the prefix for the experiment name.
        :param expname_suffix: the suffix for the experiment name.
        :param kwargs: other arguments.
        """
        arg_info_objs += ARG_SETUP_ESSENTIAL
        if not simple_args:
            arg_info_objs += ((ARG_SETUP_GENERAL if general_args else ()) +
                              (ARG_WORKSPACE_OVERRIDES if workspace_override_args else ()) +
                              (ARG_SETUP_DEEP_LEARNING_GENERAL if deep_learning_args else ()) +
                              (ARG_SETUP_NLP if nlp_args else ()))
            expname_args += (EXP_NAME_ARGS_GENERAL if general_args else ()) + (EXP_NAME_ARGS_DEEP_LEARNING_GENERAL if deep_learning_args else ())
            batchname_args += (EXP_BATCH_ARGS_GENERAL if general_args else ()) + (EXP_BATCH_ARGS_DEEP_LEARNING_GENERAL if deep_learning_args else ())
        ExperimentBase__.__init__(self,
                                  *arg_info_objs,
                                  dir_meta_data=dir_meta_data,
                                  dir_source_data=dir_source_data,
                                  dir_datasets=dir_datasets,
                                  dir_analysis=dir_analysis,
                                  dir_results=dir_results,
                                  preset_root=preset_root,
                                  preset=preset,
                                  default_workspace_root=default_workspace_root,
                                  expname_args=expname_args,
                                  expname_prefix=expname_prefix,
                                  expname_suffix=expname_suffix,
                                  batchname_args=batchname_args,
                                  batchname_prefix=batchname_prefix,
                                  batchname_suffix=batchname_suffix,
                                  **kwargs)

        # region adds convenient fields to `args`
        args = self.args
        recognized_path_prefixes = {
            '$results/': self.input_path_from_results,
            '$data/': self.input_path_from_datasets,
        }
        for arg_info_obj in arg_info_objs:
            if isinstance(arg_info_obj, ExpArgInfo):
                if arg_info_obj.could_be_path:
                    argval = getattr(args, arg_info_obj.full_name)
                    if argval:
                        for prefix, path_func in recognized_path_prefixes.items():  # TODO make this a formal mechanism
                            if argval.startswith(prefix):
                                setattr(args, arg_info_obj.full_name, path_func(target=argval[len(prefix):]))

        if not simple_args:
            setattr_if_none_or_empty__(args, 'result_path', lambda: self.output_path_to_results(target=self.name))
            setattr_if_none_or_empty__(args, 'runtime_path', lambda: paex.path_or_name_with_timestamp(path.join(args.result_path, 'run')))
            setattr_if_none_or_empty__(args, 'debug_path', lambda: path.join(args.runtime_path, 'debug'))
            setattr_if_none_or_empty__(args, 'cache_path', lambda: path.join(self.output_path_to_results(), '_cache', self.batchname))
            setattr_if_none_or_empty__(args, 'vocab_path', lambda: path.join(self.output_path_to_results(), '_vocab'))
            setattr_if_none_or_empty__(args, 'log_path', lambda: path.join(args.runtime_path, '_logs'))
            msgex.LoggableBase.__init__(self, path.join(args.log_path, 'main.log'), print_out=kwargs.get('verbose'))
            self.info_message('experiment result path', args.result_path)
            self.info_message('experiment runtime path', args.runtime_path)
            self.info_message('experiment log path', args.log_path)
            self.info_message('experiment cache path', args.cache_path)
            setattr_if_none_or_empty__(args, 'data_path', lambda: self.input_path_from_datasets())
            self.info_message('experiment data path', args.data_path)

            if general_args:
                setattr_if_none_or_empty__(args, 'test_set_path', lambda: self.input_path_from_test_sets())
                self.info_message('experiment test set path', args.test_set_path)
                setattr_if_none_or_empty__(args, 'val_set_path', lambda: self.input_path_from_val_sets())
                self.info_message('experiment validation set path', args.val_set_path)
            # region saving the arguments
            write_json(args, path.join(args.runtime_path, self.name + '.json'), indent=2)
            # endregion
        # endregion

    def get_logger(self, log_name: str, logging_level=logging.DEBUG, log_format="%(asctime)s - %(process)d - %(name)s - %(levelname)s - %(message)s", append=False, file_ext='log'):
        if hasattr(self.args, 'log_path'):
            return msgex.get_logger(name=log_name, log_dir_path=self.args.log_path, logging_level=logging_level, log_format=log_format, append=append, file_ext=file_ext)

    def get_cache(self) -> Cache:
        return get_cache(self.args)

    def get_vocab(self, vocab_name, build_mode=False, **kwargs) -> Vocabulary:
        return Vocabulary(path.join(self.args.vocab_path, vocab_name), build_mode=build_mode, **kwargs)

    def get_vocab__(self, vocab_config):
        splits = vocab_config.rsplit('-', maxsplit=2)
        if len(splits) == 1:
            return self.get_vocab(vocab_name=vocab_config)
        min_count = 1
        max_size = None
        for split in splits[1:]:
            if split.startswith('mc'):
                min_count = int(split[2:])
            elif split.startswith('ms'):
                max_size = int(split[2:])
        return self.get_vocab(vocab_name=splits[0], min_count=min_count, max_size=max_size)

    def get_init_path(self, init_ext='.th'):
        """
        Gets the initialization path. This method tries to search for an initialization path based on the following rules:
        1) if '--init' argument is specified, then
            returns the init argument if it points to an existing path;
            otherwise, searching the directory of the argument present, or the experiment's result path, to see if the '--init' argument exists as a file or sub-directory, and returns the joined path if so.
        2) otherwise, checks the directory of the preset for a file with extension name specified by `init_ext`;
            returns the file path if there is just a single path;
            otherwise, checks for a file with the main name 'best' and the extension name specified by `init_ext`, and returns the path to that file if it exists;
            otherwise, returns a file with the largest alphabetic order in the directory with the extension name.
        """
        if hasattr(self.args, 'init'):
            init = self.args.init
            if init:
                if path.exists(init):
                    return init
                if hasattr(self.args, 'preset'):  # checks if `init` is in the `preset`'s directory
                    _init = path.join(path.dirname(self.args.preset), init)
                    if path.exists(_init):
                        return _init
                _init = path.join(self.results_path(), init)
                if path.exists(_init):
                    return _init
        elif hasattr(self.args, 'preset'):
            if init_ext[0] != '.':
                init_ext = '.' + init_ext
            init_dir = path.dirname(self.args.preset)
            possible_inits = paex.get_files_by_pattern(dir_or_dirs=init_dir, pattern='*' + init_ext, full_path=True, recursive=False)
            if len(possible_inits) == 1:
                return possible_inits[0]
            elif possible_inits:
                init = path.join(init_dir, 'best' + init_ext)
                if path.exists(init):
                    return init
                possible_inits.sort(reverse=True)
                return possible_inits[0]

    def output_path_to_default_log_file(self):
        return self.output_path_to_logs(target=f'{self.name}.log')

    def input_path_from_test_sets(self, target: Union[str, Iterable] = '', multi_path=False, **kwargs) -> str:
        return get_input_path(self.args, data_type=self.dir_datasets, data_split=self.args.test_split if hasattr(self.args, 'test_split') else 'test', target=target, multipath=multi_path, **kwargs)

    def input_path_from_val_sets(self, target: Union[str, Iterable] = '', multi_path=False, **kwargs) -> str:
        return get_input_path(self.args, data_type=self.dir_datasets, data_split=self.args.val_split if hasattr(self.args, 'val_split') else 'val', target=target, multipath=multi_path, **kwargs)

    def output_path_to_test_sets(self, target: Union[str, Iterable] = '', **kwargs) -> str:
        return get_output_path(self.args, data_type=self.dir_datasets, data_split=self.args.test_split if hasattr(self.args, 'test_split') else 'test', target=target, **kwargs)

    def output_path_to_val_sets(self, target: Union[str, Iterable] = '', **kwargs) -> str:
        return get_output_path(self.args, data_type=self.dir_datasets, data_split=self.args.val_split if hasattr(self.args, 'val_split') else 'val', target=target, **kwargs)


def SimpleExperiment(*arg_info_objs,
                     dir_meta_data='meta_data',
                     dir_source_data='source_data',
                     dir_datasets='datasets',
                     dir_analysis='analysis',
                     dir_results='results',
                     default_arg_preset_root: str = None,
                     arg_preset: Union[str, dict] = None,
                     default_workspace_root='.',
                     workspace_override_args=True,
                     verbose=False, **kwargs):
    return ExperimentBase(*arg_info_objs,
                          dir_meta_data=dir_meta_data,
                          dir_source_data=dir_source_data,
                          dir_datasets=dir_datasets,
                          dir_analysis=dir_analysis,
                          dir_results=dir_results,
                          preset_root=default_arg_preset_root,
                          preset=arg_preset,
                          default_workspace_root=default_workspace_root,
                          general_args=False,
                          workspace_override_args=workspace_override_args,
                          deep_learning_args=False,
                          nlp_args=False,
                          verbose=verbose,
                          **kwargs)


# region general common arguments
_ARG_DEFAULT_TEST_SPLIT_NAME = 'test'
_ARG_DEFAULT_VAL_SPLIT_NAME = 'val'
_ARG_DEFAULT_DATA_MAX_READ = 100000000000
_ARG_DEFAULT_MULTIPROCESSING = 1
_ARG_DEFAULT_VERBOSE = True
_ARG_DEFAULT_RAND_SEED = 0
_ARG_DEFAULT_VAL_PROB = 0.1
_ARG_DEFAULT_VAL_MAX_SIZE = 5000
_ARG_DEFAULT_TEST_PROB = 0.1
_ARG_DEFAULT_TEST_MAX_SIZE = 5000
_ARG_DEFAULT_NO_FILE_CACHE = False
_ARG_DEFAULT_NO_MEMORY_CACHE = False
_ARG_DEFAULT_COMPRESS_CACHE = False
_ARG_DEFAULT_SHUFFLE_CACHE = False
_ARG_DEFAULT_LESS_EVAL_WHILE_TRAINING = True
_ARG_DEFAULT_MAX_ITER = 0
_ARG_DEFAULT_PRE_TEST_INTERVAL = 0
_ARG_DEFAULT_STAT_HARDWARE_USAGE = False
_ARG_DEFAULT_RUN_MODE = 'train'
_ARG_DEFAULT_DEBUG_MODE = False

ARG_SETUP_ESSENTIAL = (
    ExpArgInfo(full_name='verbose', default_value=_ARG_DEFAULT_VERBOSE, description='Printout/Log internal messages when applicable.'),
    ExpArgInfo(full_name='random_seed', default_value=_ARG_DEFAULT_RAND_SEED, description='The random seed for reproducible experiments.')
)
ARG_SETUP_GENERAL = (
    ExpArgInfo(full_name='test_split', default_value=_ARG_DEFAULT_TEST_SPLIT_NAME, description='The name(s) for the test data split(s); multiple data splits are supported.'),
    ExpArgInfo(full_name='val_split', default_value=_ARG_DEFAULT_VAL_SPLIT_NAME, description='The name(s) for the validation data split(s); multiple data splits are supported.'),
    ExpArgInfo(full_name='data_max_read', default_value=_ARG_DEFAULT_DATA_MAX_READ, description='Limit the maximum number of loaded data entries for experiment.', affect_batch=True, affect_vocab=True),
    ExpArgInfo(full_name='multi_processing', default_value=_ARG_DEFAULT_MULTIPROCESSING, description='The number of parallel processes for parallel processing when applicable.'),
    ExpArgInfo(full_name='test_ratio', default_value=_ARG_DEFAULT_TEST_PROB, description='The sampling ratio (probability) for the test set.'),  # TODO obsolete
    ExpArgInfo(full_name='val_ratio', default_value=_ARG_DEFAULT_VAL_PROB, description='The sampling ratio (probability) for the validation set.'),  # TODO obsolete
    ExpArgInfo(full_name='test_max_size', default_value=_ARG_DEFAULT_TEST_MAX_SIZE, description='The maximum size of the test set.'),  # TODO obsolete
    ExpArgInfo(full_name='val_max_size', default_value=_ARG_DEFAULT_VAL_MAX_SIZE, description='The maximum size of the validation set.'),  # TODO obsolete
    ExpArgInfo(full_name='init', default_value='', description='The initialization; interpretation dependent on the experiment.', could_be_path=True),
    ExpArgInfo(full_name='no_file_cache', default_value=_ARG_DEFAULT_NO_FILE_CACHE, description='Disables any file-based caching mechanism when applicable.'),
    ExpArgInfo(full_name='no_mem_cache', default_value=_ARG_DEFAULT_NO_MEMORY_CACHE, description='Disables any memory-based caching mechanism when applicable.'),
    ExpArgInfo(full_name='compress_cache', default_value=_ARG_DEFAULT_COMPRESS_CACHE, description='Compressed the cached data when applicable.'),
    ExpArgInfo(full_name='shuffle_cache', default_value=_ARG_DEFAULT_SHUFFLE_CACHE, description='The cached data items are shuffled when applicable.'),
    ExpArgInfo(full_name='less_eval_while_training', default_value=_ARG_DEFAULT_LESS_EVAL_WHILE_TRAINING, description='Serves as a signal to the experiment pipeline to fields_sub the cost of evaluation during training when applicable (e.g. no detailed model output dump).'),
    ExpArgInfo(full_name='pre_test_interval', default_value=_ARG_DEFAULT_PRE_TEST_INTERVAL, description='Sets a positive integer to enable premature test during training (i.e. evaluate on the test set before fully training the model for all epochs); '
                                                                                                        'the integer is the number of epochs between two tests. '
                                                                                                        'Set this number to 0 to disable premature tests.'),
    ExpArgInfo(full_name='max_iter', default_value=_ARG_DEFAULT_MAX_ITER, description='The maximum number of iterations. The interpretation of this number is model/experiment dependent.'),
    ExpArgInfo(full_name='stat_hardware_usage', default_value=_ARG_DEFAULT_STAT_HARDWARE_USAGE, description='The maximum number of iterations. The interpretation of this number is model/experiment dependent.'),
    ExpArgInfo(full_name='debug_mode', default_value=_ARG_DEFAULT_DEBUG_MODE, description='true to enable the debug mode; the interpretation of this argument is dependent on the experiment.'),
    ExpArgInfo(full_name='run_mode', default_value=_ARG_DEFAULT_RUN_MODE, description='The default mode for the experiment pipeline, e.g. training, evaluation, or embedding dump, etc.; the interpretation of this argument is dependent on the pipeline.')
)
EXP_NAME_ARGS_GENERAL = ('random_seed', 'data_max_read')
EXP_BATCH_ARGS_GENERAL = EXP_NAME_ARGS_GENERAL

# endregion

ARG_WORKSPACE_OVERRIDES = (
    ExpArgInfo(full_name='result_path', description='The path to the result directory. Specify this to override the workspace result path.', could_be_path=True),
    ExpArgInfo(full_name='runtime_path', description='The path to the current runtime directory. Specify this to override the workspace runtime path.', could_be_path=True),
    ExpArgInfo(full_name='log_path', description='The path to the log directory. Specify this to override the workspace log path.', could_be_path=True),
    ExpArgInfo(full_name='cache_path', description='The path to the cache directory. Specify this to override the workspace cache path.', could_be_path=True),
    ExpArgInfo(full_name='data_path', description='The path to the cache directory. Specify this to override the workspace data path.', could_be_path=True),
    ExpArgInfo(full_name='test_set_path', description='The path to the test set directory. Specify this to override the workspace test set path.', could_be_path=True),
    ExpArgInfo(full_name='val_set_path', description='The path to the validation set directory. Specify this to override the workspace validation set path.', could_be_path=True)
)

# region deep learning common arguments
_ARG_DEFAULT_GPU_INDEX = '*'
_ARG_DEFAULT_BATCH_SIZE = 64
_ARG_DEFAULT_BATCH_CACHE_GROUP_SIZE = 64
_ARG_DEFAULT_TEST_BATCH_SIZE = -1
_ARG_DEFAULT_EPOCHS = 20
_ARG_DEFAULT_LEARNING_RATE = 0.0004
_ARG_DEFAULT_REG_LAMBDA = 1E-4
_ARG_DEFAULT_MODEL_TEST_TOP_K = 3
_ARG_DEFAULT_WEIGHT_DECAY = 5E-4
_ARG_DEFAULT_DROPOUT = 0
_ARG_DEFAULT_EARLY_STOP_PATIENCE = 10
_ARG_DEFAULT_INPUT_STATE_DIM = 300
_ARG_DEFAULT_HIDDEN_STATE_DIM = [512, 256, 128]
_ARG_DEFAULT_MAX_SEQ_LEN = 12
_ARG_DEFAULT_SIMILARITY_LAYER = 'dot'
_ARG_DEFAULT_POOL_LAYER = 'sum'
_ARG_DEFAULT_EMBEDDING_SAVE_OPTION = 'numpy'
_ARG_DEFAULT_OUTPUT_SAVE_RATIO = 0.0
_ARG_DEFAULT_MODEL_SAVE_RATIO = 0.5
_ARG_DEFAULT_MODEL_SAVE_MIN = 3
ARG_SETUP_DEEP_LEARNING_GENERAL = (
    ExpArgInfo(full_name='gpu', default_value=_ARG_DEFAULT_GPU_INDEX, description='The index of the GPU to use.'),
    ExpArgInfo(full_name='batchsize', default_value=_ARG_DEFAULT_BATCH_SIZE, description='The training batch size.', affect_batch=True),
    ExpArgInfo(full_name='test_batchsize', default_value=_ARG_DEFAULT_BATCH_SIZE, description='The test batch size.', affect_batch=True),  # TODO at test time, the batchsize for the batch cache should be according to this parameter
    ExpArgInfo(full_name='epochs', default_value=_ARG_DEFAULT_EPOCHS, description='The number of training epochs'),
    ExpArgInfo(full_name='learning_rate', default_value=_ARG_DEFAULT_LEARNING_RATE, description='The learning rate. Rule of thumb: higher rate for simpler models; lower rate for complex models.'),
    ExpArgInfo(full_name='reg_lambda', default_value=_ARG_DEFAULT_REG_LAMBDA, description='The regularization penalty.'),
    ExpArgInfo(full_name='weight_decay', default_value=_ARG_DEFAULT_WEIGHT_DECAY, description='The optimization weight decay.'),
    ExpArgInfo(full_name='dropout', default_value=_ARG_DEFAULT_DROPOUT, description='The optimization weight decay.'),
    ExpArgInfo(full_name='early_stop_patience', default_value=_ARG_DEFAULT_EARLY_STOP_PATIENCE, description='During training, if the average performance declines for this number of previous epochs, '
                                                                                                            'then the early-stop is activated to terminate the entire training process without finishing the remaining epochs; '
                                                                                                            'to disable early-stop, assign 0 to this parameter'),
    ExpArgInfo(full_name='model_test_top_k', default_value=_ARG_DEFAULT_MODEL_TEST_TOP_K, description='Test the top-k models on the test sets.'),
    ExpArgInfo(full_name='input_state_dim', default_value=_ARG_DEFAULT_INPUT_STATE_DIM, description='The dimension of the input states (embedding).'),
    ExpArgInfo(full_name='hidden_state_dim', default_value=_ARG_DEFAULT_HIDDEN_STATE_DIM, description='The dimension of the hidden states (embeddings). May specify a list of integers for multiple layers of hidden state dimensions.'),
    ExpArgInfo(full_name='max_seq_len', default_value=_ARG_DEFAULT_MAX_SEQ_LEN, description='The maximum sequence length needed for sequence models.', affect_batch=True, affect_vocab=True),
    ExpArgInfo(full_name='similarity_layer', default_value=_ARG_DEFAULT_SIMILARITY_LAYER, description='The similarity function used in the neural network when applicable. A similarity layer is common in information retrieval models.'),
    ExpArgInfo(full_name='pool_layer', default_value=_ARG_DEFAULT_POOL_LAYER, description='The pooling layer used in the neural network when applicable. A pooling layer is common in NLP and computer vision models.'),
    ExpArgInfo(full_name='batch_cache_group_size', default_value=_ARG_DEFAULT_BATCH_CACHE_GROUP_SIZE, description='The number of batches in one cache group; (e.g. for a file cache, a cache group is typically written in a cache file).', affect_batch=True),
    ExpArgInfo(full_name='embedding_save_option', default_value=_ARG_DEFAULT_EMBEDDING_SAVE_OPTION, description='The options for embedding save. Currently support: 1) original, the embedding is saved as it is; 2) cpu, the embedding is saved as CPU tensors; 3) numpy, the embedding is saved as numpy arrays.'),
    ExpArgInfo(full_name='output_save_ratio', default_value=_ARG_DEFAULT_OUTPUT_SAVE_RATIO, description='Specifies the ratio of outputs to save when applying the model on the input data. The primary purpose of this parameter is to extract a percentage of model output for visualization or analysis.'),
    ExpArgInfo(full_name='model_save_ratio', default_value=_ARG_DEFAULT_OUTPUT_SAVE_RATIO, description='Specifies the ratio of models to save during the training. Typically, this ratio is with respect to the number of epochs. For example, when training for 10 epochs, and this ratio is 0.5, then we save models for the last 5 epochs.'),
    ExpArgInfo(full_name='model_save_min', default_value=_ARG_DEFAULT_MODEL_SAVE_MIN, description='Specifies the minimum number of models to save during the training. Use this parameter together with `model_save_ratio` to determine the actual number of saved models.')
)
EXP_BATCH_ARGS_DEEP_LEARNING_GENERAL = ('max_seq_len', 'batchsize', 'batch_cache_group_size')
EXP_NAME_ARGS_DEEP_LEARNING_GENERAL = ('epochs', 'learning_rate', 'dropout') + EXP_BATCH_ARGS_DEEP_LEARNING_GENERAL

ARG_SETUP_NLP = ()


# endregion

class ExpUnitData:
    __slots__ = ('data_reader', 'iterator')


class ExpUnitModel:
    __slots__ = ('model', 'input_func', 'loss_func', 'optimizer', 'output_func')


class ExpUnitEvaluation:
    __slots__ = ('metrics', 'metric_func', 'analyzers')


class ExpUnitNLP:
    __slots__ = ('tokenizer_indexers', 'vocab', 'to_instance')
